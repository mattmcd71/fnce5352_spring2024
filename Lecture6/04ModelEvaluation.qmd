---
title: "Model Evaluation"
author: "Matthew McDonald"
format: 
  revealjs:
    slide-number: true
execute: 
  echo: true
  eval: true
  warning: false
editor: source
---

```{r}
#| include: false
#| file: setup.R
```

## Regression vs Classification {.smaller}

-   Definition:

    -   **Regression**: Predicts continuous outcomes. It estimates a mapping function (f) from input variables (X) to a continuous output variable (Y).

    -   **Classification**: Predicts discrete outcomes. It categorizes input data into two or more classes.

-   Output Type:

    -   **Regression**: The output is a real or continuous value (e.g., salary, price).

    -   **Classification**: The output is a category (e.g., default or no default).

-   Both regression and classification are types of supervised machine learning algorithms, where a model is trained according to the existing model along with correctly labeled data

## RMSE {.smaller}

-   Definition: The square root of the average of the squared differences between the predicted and actual values. It measures the standard deviation of the residuals (prediction errors).

-   Interpretation: Represents the average error made by the model in predicting the outcome. Lower values indicate better fit.

-   Scale: Depends on the target variable's scale. Higher RMSE values may not necessarily indicate a poor model, especially if the target variable has a wide range.

-   Sensitivity: More sensitive to outliers than other metrics, such as MAE (Mean Absolute Error).

## RMSE Formula

-   $\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$

-   $n$ is the number of observations.

-   $y_i$ is the actual value of the observation.

-   $\hat{y}_i$ is the predicted value.

-   The sum $\sum_{i=1}^{n} (y_i - \hat{y}_i)^2$ calculates the squared differences between the actual and predicted values.

## $R^2$ {.smaller}

-   Definition: The proportion of the variance in the dependent variable that is predictable from the independent variables. It is a statistical measure of how close the data are to the fitted regression line.

-   Interpretation: Values range from 0 to 1. A higher value indicates a better fit, with 1 meaning the model explains all the variability of the response data around its mean.

-   Scale-Independent: Unlike RMSE, R-squared is a normalized measure, making it easier to compare the goodness of fit across different datasets and models.

-   Limitation: Can be misleadingly high in models with many predictors or when using higher-order polynomials. Adjusted R-squared is often used to account for the number of predictors.

## $R^2$ Formula

-   $R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$
-   $n$, $y_i$ and $\hat{y}_i$ are the defined the same as in $RMSE$
-   The numerator, $\sum_{i=1}^{n} (y_i - \hat{y}_i)^2$, is the sum of the squared differences between the actual and the predicted values (also known as the sum of squares of residuals).
-   The denominator, $\sum_{i=1}^{n} (y_i - \bar{y})^2$, is the total sum of squares (TSS) or the sum of the squared differences between the actual values and the mean of the actual values.

## RMSE vs $R^2$

$RMSE$ measures *accuracy* while $R^2$ measures *correlation*.

```{r performance-reg-metrics, echo = FALSE}
#| fig.cap = "Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination",
#| fig.alt = "Scatter plots of numeric observed versus predicted values for models that are optimized using the RMSE and the coefficient of determination. The former results in results that are close to the 45 degree line of identity while the latter shows results with a tight linear correlation but falls well off of the line of identity."
library(tidymodels)
library(kableExtra)
library(modeldata)
tidymodels_prefer()
source("ames_snippets.R")
load("RData/lm_fit.RData")

set.seed(234)
n <- 200
obs <- runif(n, min = 2, max = 20)

reg_ex <- 
  tibble(
    observed = c(obs, obs),
    predicted = c(obs + rnorm(n, sd = 1.5), 5 + .5 * obs + rnorm(n, sd = .5)),
    approach = rep(c("RMSE optimized", "R^2 optimized"), each = n)
  ) %>% 
  mutate(approach = factor(
    approach, 
    levels = c("RMSE optimized", "R^2 optimized"),
    labels = c(expression(RMSE ~ optimized), expression(italic(R^2) ~ optimized)))
  )

ggplot(reg_ex, aes(x = observed, y = predicted)) + 
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  coord_obs_pred() + 
  facet_wrap(~ approach, labeller = "label_parsed")
```

## Calculating RMSE using yardstick

```{r}
#| echo: false
library(tidymodels)
library(countdown)
library(tidyverse)
data("tree_frogs", package = "stacks")
tree_frogs <- tree_frogs %>%
  mutate(t_o_d = factor(t_o_d),
         age = age / 86400) %>% 
  filter(!is.na(latency)) %>%
  select(-c(clutch, hatched))

set.seed(123)
frog_split <- initial_split(tree_frogs, prop = 0.8, strata = latency)
frog_train <- training(frog_split)
frog_test <- testing(frog_split)
tree_spec <- decision_tree(cost_complexity = 0.001, mode = "regression")
tree_wflow <- workflow(latency ~ ., tree_spec)
tree_fit <- fit(tree_wflow, frog_train)
data_with_predictions <- augment(tree_fit, new_data = frog_test)
```

```{r}
library (yardstick)

rmse(data_with_predictions, truth = latency, estimate = .pred)
```

## Calculating Multiple Metrics

```{r}
metrics <- metric_set(rmse, rsq, mae)
metrics(data_with_predictions, truth = latency, estimate = .pred)
```

. . .

-   RMSE: difference between the predicted and observed values ⬇️
-   $R^2$: squared correlation between the predicted and observed values ⬆️
-   MAE: similar to RMSE, but mean absolute error ⬇️

## Classification Metrics {.smaller}

**Definitions**:

-   **Hard Predictions**: provide the final classification result directly, indicating the class to which the input data is most likely to belong, without showing any uncertainty or probability of the decision.

    -   Example: A model predicts an obligor is either "defaulter" (1) or "not a defualter" (0), with no indication of how likely it is to be a defaulter.

-   **Soft Predictions**: provide a probability distribution over all possible classes, indicating the likelihood that the input data belongs to each class.

    -   Example: A model predicts an obligor has a 90% probability of being a "defaulter" and a 10% probability of being "not a defaulter".

## Confusion Matrix

For Hard Predictions, we can create a confusion matrix

![](images/confusionmatrix.webp){fig-align="center"}

## Components of Confusion Matrix

1.  **True Positives (TP):** Instances correctly predicted as positive.

2.  **True Negatives (TN):** Instances correctly predicted as negative.

3.  **False Positives (FP):** Instances incorrectly predicted as positive, also known as Type I error.

4.  **False Negatives (FN):** Instances incorrectly predicted as negative, also known as Type II error.

## Derived Metrics from Confusion Matrix {.smaller}

From the confusion matrix, several important metrics can be calculated, including:

-   **Accuracy:** Overall correctness of the model. $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$

-   **Precision:** Proportion of positive identifications that were actually correct. $Precision=\frac{TP}{TP+FP}$

-   **Recall (Sensitivity or True Positive Rate):** Proportion of actual positives that were identified correctly. $Recall=\frac{TP}{TP+FN}$

-   **Specificity (True Negative Rate):** Proportion of actual negatives that were identified correctly. $Specificity=\frac{TN}{TN+FP}$

-   **F1 Score:** Harmonic mean of precision and recall, providing a single metric to assess the balance between them. $F1=2 × \frac{Precision * Recall}{Precision + Recall}$

## Mapping Soft Predictions into Hard Predictions {.smaller}

::: columns
::: {.column width="50%"}
For two classes, the customary cutoff to map probabilities into predictions is 50%. If the probability of class 1 is \>= 50%, they would be labelled as *Class1*

What happens if you change the cutoff?

-   Increasing makes it harder to be called *Class1* $\Longrightarrow$ fewer predicted events, specificity $\Uparrow$, sensitivity $\Downarrow$

-   Decreasing makes it harder to be called *Class1* $\Longrightarrow$ fewer predicted events, specificity $\Uparrow$, sensitivity $\Downarrow$
:::

::: {.column width="50%"}
With two classes, the **Receiver Operating Characteristic (ROC) curve** can be used to estimate performance using a combination of sensitivity and specificity.

To create the curve, many alternative cutoffs are evaluated.

For each cutoff, we calculate the sensitivity and specificity.

The ROC curve plots the sensitivity (True Positive Rate) versus 1 - specificity (False Positive Rate)
:::
:::

## The ROC Curve

![](images/roc_curve.png){fig-align="center"}

## AUC

**Integral Relationship**: The AUC (Area Under the Curve) represents the integral or the total area under the ROC (Receiver Operating Characteristic) curve. It quantifies the overall ability of the model to discriminate between positive and negative classes across all possible thresholds, with a higher AUC indicating better model performance and discrimination capability.

## Classification Metrics with yardstick

```{r}
data(two_class_example)
tibble(two_class_example)
```

## Hard Prediction Metrics

::: columns
::: {.column width="50%"}
```{r}
# A confusion matrix: 
conf_mat(two_class_example, 
         truth = truth, 
         estimate = predicted)
```
:::

::: {.column width="50%"}
```{r}
cm <- metric_set(accuracy, 
                 sens, 
                 spec)
cm(two_class_example, 
   truth = truth, 
   estimate = predicted)
```
:::
:::

## ROC Curve

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve %>% autoplot()
```
## Calculating AUC

```{r}
roc_auc(two_class_example, truth, Class1)
```

